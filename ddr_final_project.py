# -*- coding: utf-8 -*-
"""ddr_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ueLkDl4lrWGn6Ka3V5wrzKxNcMrPljcb

# BAX 422 - Data Design and Representation - Final Project

## Group 12: Kuan-Yu (Leo) Liao, Rishikesan (Rishi) Ravichandran, Anurag Vedagiri
"""

import praw
import json
import configparser
import time
from dotenv import load_dotenv
from datetime import datetime, timedelta
import os
import json
import pandas as pd
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from pymongo import MongoClient
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import string
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

"""## Web Scraping"""

def connect_mongo(name,collection):
    '''
    Function to create Mongodb database and collection if not exists.
    '''
    #Create a connection to the local mongodb server instance
    client = MongoClient("localhost", 27017)

    db = client[name]
    collection = db[collection]

    return client, db, collection

'''
***** USER INPUT *****

Provide the information or the league you are interested in analysing. Please note that the name should be the one that you are looking
for in your sub-reddit, as the same name will be used for MongoDB database creations. Although, other scraping sources will also be using the
same database, sub-reddit's name will be the base database name for form of data related to a particular league.

For, each instance, in the database, we'll have multiple collections such as reddit, espn etc.

Provide the the match here. The words should only contain single most word that describe each team better, which is also widely accpeted
by fans.

For instance, the match can be "Yankees vs Mets" instead of "New York Yankees vs New York Mets".
'''

#Input here!! Note. This is the only thing that has to be changed for different matches & different leagues

league_name = "baseball" #Note. This must be similar to the league or sports that you're looking into, and have a subreddit
match = "Yankees vs Dodgers"

#This part of code should always be running on the match day just before the start of the match for the latest information

db_name=league_name; collection_name = f"{match.lower().replace(' ','-')}-{datetime.now().date()}"

client, db, collection = connect_mongo(db_name, collection_name)
# scrap_reddit(match,collection)

load_dotenv()

#Get the reddit api credenials set in the environment. To do this, go to reddit APIs, and get the credentials for your account.
#And create a .env file with all these fields, and their values, and place it in the same directory as the code files.
reddit = praw.Reddit(user_agent=os.getenv('USER_AGENT'),
                     client_id=os.getenv('CLIENT_ID'),
                     client_secret=os.getenv('CLIENT_SECRET'),
                     username=os.getenv('USERNAME'),
                     password=os.getenv('PASSWORD'))

subreddit = reddit.subreddit(league_name)

def scrape_reddit(match,collection):
    """
    Function to scrape data from reddit through Praw API.
    """

    keywords = [team.lower() for team in match.split(' vs ')]

    #Get the current time and subtract 3 days from it, as we'll only be scraping the last 3 days of data
    three_days_ago = (datetime.now() - timedelta(days=3)).timestamp()

    # #Create a dictionary to store all the data
    # data = {}

    #Get the posts based on the keywords generated from the match, i.e only the recent 300 which are also within last 3 days
    for post in subreddit.new(limit=300):

        #Stop scraping if the post is older
        if post.created_utc < three_days_ago:
            break

        #Get the title information for the post
        title = post.title.lower()

        #Check for the post titles that contains our keywords
        if any(key in title for key in keywords):
            # sleep for 5 seconds to avoid hitting the API too hard, this will be only before fetching details of a post
            time.sleep(5)

            #Get access to the post with it's id through the API
            post_obj = reddit.submission(id=post.id)

            #Get the post's title
            title = post_obj.title

            #Get the post's description
            description = post_obj.selftext

            #Now we'll try to fetch all the comments of the post with no limit for the 'More Comments' in Reddit
            post_obj.comments.replace_more(limit=None)

            comments = []

            #Get all the comments of the post
            for comment in post_obj.comments.list():
                comments.append(comment.body)

            #Add the post's data to the dictionary
            data = {}
            data = {
                "title":title, #Title of the post
                "description": description, # Description of the post
                "comments": comments, #An array of possible strings
                "source": "reddit",
                "time":post.created_utc
            }

            doc_insert = collection.insert_one(data)

def scrape_espn(match,collection):
    '''
    Function to scrape data from ESPN.
    '''

    #Create a driver object for the chrome
    driver = webdriver.Chrome()

    #We'll wait for 5 sec before sending each request

    #Go to ebay.com
    time.sleep(5)
    driver.get('https://espn.com/')

    #search for "Cell Phones"
    time.sleep(5)
    search = driver.find_element(By.CSS_SELECTOR,'a[id="global-search-trigger"]')
    search.click()

    time.sleep(5)
    input=driver.find_element(By.CSS_SELECTOR,'input[id="global-search-input"]')
    input.send_keys(f'{match} \n')

    #Click the articles filter
    time.sleep(5)
    articles = driver.find_element(By.XPATH, '//a[text()="Articles" and @data-track-filtername="articles"]')
    articles.click()

    #Check for the recent articles based on similar timeframe as reddit, default: 3 days
    #li.class_='time-elapsed', re.compile(r'(\d{1,24}h|[1-3]d)',li.text)

    #Get the page source
    page_html = driver.page_source

    soup = BeautifulSoup(page_html,'html.parser')

    #Just take the first article for now. This will be ideally done based on the timeframe given gloabally i.e for both reddit and espn.
    article_url="https://www.espn.com"+soup.find_all('ul',class_='article__Results')[0].find_all('a')[0].get('href')

    time.sleep(5)
    page_article = requests.get(article_url, headers = {'User-agent': 'Mozilla/5.0'})
    soup_article = BeautifulSoup(page_article.content,'html.parser')

    #Title of the article
    title = soup_article.find_all('h1')[0].text

    #Description of the article
    description=""
    for p in range(len(soup_article.find_all('p'))):
        description+=soup_article.find_all('p')[p].text

    #Timestamp of the published article
    article_time = soup_article.find_all('span',class_='timestamp')[-2].text

    #Add the article's data to the dictionary
    data = {}
    data = {
        "title":title, #Title of the article
        "description": description, # Description of the article
        "source": "espn",
        "time": article_time
    }
    print("Sample Article Document shown below:\n")
    print(data)
    doc_insert = collection.insert_one(data)

'''
This part is still work in progress, as there are some minor issues while scraping articles from ESPN.
'''

scrape_espn(match,collection)

"""## Data Transformation"""

def transform_label_reddit_data(match,data):
    '''
    This function will transform the raw data into a dataframe, where every row will be a english sentence from the title/comments/
    description of a post, labelling to the keywords from the match string.

    The rules are based on assigning a sentence text to a keyword only if one of the keyword exists in that sentence. If both keywords
    exists in a sentence, that sentence will be ignored for now.

    However, in future, a more appropriate method can be enforced to accomodate those as well.
    '''

    keywords = [team.lower() for team in match.split(' vs ')]

    #Structure the raw data into a dataframe, every sentence will be a row in the dataframe
    row=[]

    for doc in doc_list:
        title=doc['title']
        description = doc['description']
        comments = doc['comments']

        #Combine all the messages of a post i.e it's title, description, and comments into an array
        messages=[title,description]+comments

        for message in messages:
            message=message.lower()

            #Check if the message contains any of the keywords
            if keywords[0] in message and keywords[1] not in message:
                row.append({"team":keywords[0],"message":message})
            elif keywords[1] in message and keywords[0] not in message:
                row.append({"team":keywords[1],"message":message})

    #Now create the dataframe
    return pd.DataFrame(row)

#If the code is executed on the same day as the scraping, the below collection_name variable assignment is not required.
#In general, it is good to keep in mind that it is better to run both scraping and analysis on the match day successively.

# collection_name = "yankees-vs-dodgers-2024-03-18"

doc_list=[doc for doc in client[db_name][collection_name].find({"source":"reddit"})]
data_fans = transform_label_reddit_data(match, doc_list)

data_fans

"""## Data Preprocessing"""

#We'll use lemmatizer to get the basic form of a word without tense
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    #First we'll lowercase the text
    text = text.lower()

    #Now remove the punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    #We'll now tokenize the text to words
    words = word_tokenize(text)

    #Now remove stopwards, and lemmatize them
    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]
    return ' '.join(words)

#Processing the originals messages
data_fans['message'] = data_fans['message'].apply(preprocess_text)

data_fans

"""# Data Exploration"""

#We'll use Vectorizer takes the row text and count them and put into a matrix
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(data_fans['message'])

#Count the occurence of each word
word_freq = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out()).sum()

#Now get the top 30 words
top_30_words = word_freq.nlargest(30)

#Display the top 30 words
top_30_words.plot(kind='bar')
plt.title(f"Top words in Reddit during {match}")
plt.show()

"""# Data Model Building

### Model 1 - Bag of Words (Fans - Reddit)
"""

#We'll define the sentiment score manually here, ideally this should be automated or have a pre-fixed dict in future
#if Bag of words is implemented

#Assign the +ve or -ve sentiments to words observed and typical words in sports, ad-hoc method**
sentiment_words = {'money':1,'trade':-1,'minor':-1,'contract':-1,'deal':1,'last':-1,'strength':1,'win':1,'lose':-1}

#Count only the words in sentiment_words
vectorizer_sentiment_scores = CountVectorizer(vocabulary=sentiment_words.keys())
X_sentiment_scores = vectorizer_sentiment_scores.fit_transform(data_fans['message'])

#Now convert to dataframe
df_sentiment_scores = pd.DataFrame(X_sentiment_scores.toarray(), columns=vectorizer_sentiment_scores.get_feature_names_out())

#Now multiply each word by its corresponding sentiment score
for word, score in sentiment_scores.items():
    df_sentiment_scores[word] *= score

#Add the columns to our original dataframe
data_fans = pd.concat([data_fans, df_sentiment_scores], axis=1)

data_fans

#Add a new column with the total sentiment score for the row test
data_fans['sentiment_score'] = data_fans.iloc[:,2:].sum(axis=1)

#Label the row as positive/negative/neutral
data_fans['sentiment_label']= data_fans['sentiment_score'].apply(lambda x: "positive" if x>0 else ("negative" if x < 0 else "neutral"))

#Now group by each 'team' and compute sentiment proportions
sentiment_props = data_fans.groupby('team')['sentiment_label'].value_counts().unstack().fillna(0)

#Now we'll calculate the sentiment proportions
sentiment_props['total'] = sentiment_props.sum(axis=1)
sentiment_props['%positive'] = sentiment_props['positive'] / sentiment_props['total'] * 100
sentiment_props['%negative'] = sentiment_props['negative'] / sentiment_props['total'] * 100
sentiment_props['%neutral'] = sentiment_props['neutral'] / sentiment_props['total'] * 100

#Now calculate the win percentage, for now it is same % positive, but in future a different formula can be used
sentiment_props['win_percentage'] = sentiment_props['%positive'] / sentiment_props['total'] * 100

#Now normalize the win percentages so they add up to 100% i.e to find the odds with this
total_win_percentage = sentiment_props['win_percentage'].sum()
sentiment_props['normalized_win_percentage'] = sentiment_props['win_percentage'] / total_win_percentage * 100

#Final match odds results
sentiment_counts.iloc[:,4:]

"""### Model to look out for in future work!!

### Model 2 - Bag of Words (Critics: ESPN)

The idea is to use similar approach as the one explained under model 1, but with ESPN (critics) data.

### Model 3 - Bag of Words (Ensemble of Fans and Critics: Reddit & ESPN)

The idea is to use an ensemble approach of model 1 and model 2.

### Model 4 - Sentiment Analysis (Fine tuning LLMs like llama2, GPT-3 etc)

The idea is fine tune a LLM model with a large collection of our scraped data. However, it would be more ideal if we are able to label them
all before the training.

# The End
"""